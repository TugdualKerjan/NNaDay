{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to implement the VQVAE mentionned in the paper here: https://arxiv.org/pdf/1711.00937\n",
    "\n",
    "    As a first experiment we compare VQ-VAE with normal VAEs (with continuous variables), as well as    VIMCO [ 28 ] with independent Gaussian or categorical priors. We train these models using the same\n",
    "    standard VAE architecture on CIFAR10, while varying the latent capacity (number of continuous or\n",
    "    discrete latent variables, as well as the dimensionality of the discrete space K).\n",
    "    \n",
    "    \n",
    "    The encoder consists of:\n",
    "    \n",
    "    - 2 strided convolutional layers with stride 2 and window size 4 × 4\n",
    "    - two residual 3 × 3 blocks (implemented as ReLU, 3x3 conv, ReLU, 1x1 conv), all having 256 hidden units.\n",
    "\n",
    "    The decoder similarly has:\n",
    "    \n",
    "    - two residual 3 × 3 blocks, followed by\n",
    "    - two transposed convolutions with stride 2 and window size 4 × 4. \n",
    "    \n",
    "    We use the ADAM optimiser [21 ] with learning rate 2e-4 and evaluates the performance after 250,000 steps with batch-size 128. For VIMCO we use 50 samples in the multi-sample training objective\n",
    "\n",
    "Unfortunately it doesn't seem like they specify the order of the residual block placement, i.e. when the identity function is added. We'll suppose it's at the end.\n",
    "\n",
    "Basing myself off https://github.com/google-deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py for the code which is the official implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import equinox as eqx\n",
    "import equinox.nn as nn\n",
    "\n",
    "class ResBlock(eqx.Module):\n",
    "    layers: list\n",
    "    norm1: nn.BatchNorm\n",
    "    norm2: nn.BatchNorm\n",
    "\n",
    "    def __init__(self, dim, key):\n",
    "        key1, key2 = jax.random.split(key, 2)\n",
    "\n",
    "        self.layers = [\n",
    "            nn.Conv2d(dim, dim, (5, 5), padding=\"SAME\", key=key1),\n",
    "            nn.Conv2d(dim, dim, (5, 5), padding=\"SAME\", key=key2)\n",
    "        ]\n",
    "        self.norm1 = nn.BatchNorm(input_size=dim, axis_name=\"batch2\", momentum=0.9, dtype=jax.numpy.float32)\n",
    "        self.norm2 = nn.BatchNorm(input_size=dim, axis_name=\"batch2\", momentum=0.9, dtype=jax.numpy.float32)\n",
    "\n",
    "    def __call__(self, x, state):\n",
    "        y = x\n",
    "\n",
    "        y = self.layers[0](y)\n",
    "        y, state = self.norm1(y, state)\n",
    "\n",
    "        y = jax.nn.relu(y)\n",
    "\n",
    "        y = self.layers[1](y)\n",
    "        y, state = self.norm2(y, state)\n",
    "\n",
    "        y = y + x\n",
    "        y = jax.nn.relu(y)\n",
    "\n",
    "        return y, state\n",
    "\n",
    "\n",
    "class Encoder(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, dim, key):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "\n",
    "        self.layers = [\n",
    "            nn.Conv2d(1, 2, (4, 4), 2, padding=\"SAME\", key=key1),\n",
    "            nn.Conv2d(2, 4, (4, 4), 2, padding=\"SAME\", key=key3),\n",
    "            ResBlock(dim, key=key2),\n",
    "            ResBlock(dim, key=key4)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x, state):\n",
    "        y = x\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ResBlock):\n",
    "                y, state = layer(y, state)\n",
    "            else:\n",
    "                y = layer(y) \n",
    "    \n",
    "        return y, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "@eqx.filter_jit\n",
    "def forward(model, x, state):\n",
    "    return jax.vmap(model, axis_name=\"batch2\", in_axes=(0, None), out_axes=(0, None))(x, state)\n",
    "\n",
    "@eqx.filter_grad(has_aux=True)\n",
    "@eqx.filter_jit\n",
    "def loss(model, x, y, state):\n",
    "    result, state = forward(model, x, state)\n",
    "    loss = jax.numpy.mean(jax.numpy.abs(result - y))\n",
    "    return loss, state\n",
    "\n",
    "\n",
    "key1, key2, key3 = jax.random.split(jax.random.PRNGKey(69), 3)\n",
    "\n",
    "model, state = eqx.nn.make_with_state(Encoder)(dim=4, key=key1)\n",
    "\n",
    "optimizer = optax.adam(1e-5)\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "x = jax.random.normal(key2, (10, 1, 100, 100))\n",
    "y = jax.random.normal(key3, (10, 1, 25, 25))\n",
    "\n",
    "grads = loss(model, x, y, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the convolutional layers only divide by 4x4=16 the embedded picture in terms of dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class Decoder(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, dim, key):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "\n",
    "        self.layers = [\n",
    "            ResBlock(dim, key=key2),\n",
    "            ResBlock(dim, key=key4),\n",
    "            nn.ConvTranspose2d(4, 2, (4, 4), 2, padding=\"SAME\", key=key1),\n",
    "            nn.ConvTranspose2d(2, 1, (4, 4), 2, padding=\"SAME\", key=key3),\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x, state):\n",
    "        y = x\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ResBlock):\n",
    "                y, state = layer(y, state)\n",
    "            else:\n",
    "                y = layer(y)\n",
    "\n",
    "        y = jax.nn.sigmoid(y)\n",
    "    \n",
    "        return y, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def forward(model, x, state):\n",
    "    return jax.vmap(model, axis_name=\"batch\", in_axes=(0, None), out_axes=(0, None))(x, state)\n",
    "\n",
    "@eqx.filter_grad(has_aux=True)\n",
    "@eqx.filter_jit\n",
    "def loss(model, x, y, state):\n",
    "    result, state = forward(model, x, state)\n",
    "    loss = jax.numpy.mean(jax.numpy.abs(result - y))\n",
    "    return loss, state\n",
    "\n",
    "key1, key2, key3 = jax.random.split(jax.random.PRNGKey(69), 3)\n",
    "\n",
    "model, state = eqx.nn.make_with_state(Decoder)(key1)\n",
    "\n",
    "x = jax.random.normal(key2, (10, 256, 100, 100))\n",
    "y = jax.random.normal(key3, (10, 256, 25, 25))\n",
    "\n",
    "grads = loss(model, y, x, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a variational autoencoder - It takes in inputs, autoencodes them into a latent space but instead of poping out simply a vector it should also pop out two values to describe the distribution of that input: mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import equinox as eqx\n",
    "import jax.numpy as np\n",
    "\n",
    "class Quantizer(eqx.Module):\n",
    "    K: int\n",
    "    D: int\n",
    "    codebook: np.array\n",
    "\n",
    "    def __init__(self, num_vecs, num_dims, key):\n",
    "        self.K = num_vecs\n",
    "        self.D = num_dims\n",
    "\n",
    "        # Init a matrix of vectors that will move with time\n",
    "\n",
    "        self.codebook = jax.nn.initializers.variance_scaling(scale=1.0, mode=\"fan_in\", distribution=\"uniform\")(key, (self.K, self.D))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # X comes in as a N x D Matrix.\n",
    "        flattened_x = jax.numpy.reshape(x, (-1, self.D))\n",
    "        # Calculate dist\n",
    "        # Nx1\n",
    "        a_squared = np.sum(flattened_x**2, axis=-1, keepdims=True)\n",
    "        # 1xK\n",
    "        b_squared = np.transpose(np.sum(self.codebook**2, axis=-1, keepdims=True))\n",
    "        distance = a_squared + b_squared - 2*np.matmul(flattened_x, np.transpose(self.codebook))\n",
    "\n",
    "\n",
    "        encoding_indices = np.reshape(\n",
    "            np.argmin(distance, axis=-1), x.shape[0]\n",
    "        )\n",
    "\n",
    "\n",
    "        z_q = self.codebook[encoding_indices]\n",
    "\n",
    "\n",
    "        z_q = flattened_x + jax.lax.stop_gradient(z_q - flattened_x) # For the straight through estimation.\n",
    "        z_q = jax.numpy.reshape(z_q, (4, 7, 7))\n",
    "\n",
    "        return z_q\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the above does is create K vectors in D dimensional space. Incoming vectors find their nearest match and the loss calculated is the L2 distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "\n",
    "# @jax.jit\n",
    "def forward(model, x):\n",
    "    return jax.vmap(model)(x)\n",
    "\n",
    "# @jax.jit\n",
    "def loss(model, x):\n",
    "    y = forward(model, x)\n",
    "    \n",
    "    los = np.mean((x - y) ** 2)\n",
    "    # print(los)\n",
    "    return los\n",
    "\n",
    "key1, key2 = jax.random.split(jax.random.PRNGKey(69), 2)\n",
    "\n",
    "model = Quantizer(10, 7*7, key=key1)\n",
    "x = jax.random.normal(key2, (64, 4, 7, 7))\n",
    "\n",
    "loss = jax.grad(loss, allow_int=True)\n",
    "\n",
    "grads = loss(model, x)\n",
    "print(grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxtts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
