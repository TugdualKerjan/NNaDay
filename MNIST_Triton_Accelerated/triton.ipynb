{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver\n",
    "\n",
    "def is_hip():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n",
    "\n",
    "def is_cdna():\n",
    "    return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942', 'gfx90a', 'gfx908')\n",
    "\n",
    "def naive_softmax(x: torch.Tensor):\n",
    "    x_max = x.max(dim=1)[0]\n",
    "    z = x - x_max[:, None]\n",
    "    num = torch.exp(z)\n",
    "    den = num.sum(dim=1)\n",
    "    ret = num / den[:, None]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,\n",
    "                   num_stages: tl.constexpr):\n",
    "    # starting row of the program\n",
    "    row_start = tl.program_id(0)\n",
    "    row_step = tl.num_programs(0)\n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):\n",
    "        # The stride represents how much we need to increase the pointer to advance 1 row\n",
    "        row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "        # The block size is the next power of two greater than n_cols, so we can fit each\n",
    "        # row in a single block\n",
    "        col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "        input_ptrs = row_start_ptr + col_offsets\n",
    "        # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n",
    "        mask = col_offsets < n_cols\n",
    "        row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n",
    "        # Subtract maximum for numerical stability\n",
    "        row_minus_max = row - tl.max(row, axis=0)\n",
    "        # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n",
    "        numerator = tl.exp(row_minus_max)\n",
    "        denominator = tl.sum(numerator, axis=0)\n",
    "        softmax_output = numerator / denominator\n",
    "        # Write back output to DRAM\n",
    "        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "        output_ptrs = output_row_start_ptr + col_offsets\n",
    "        tl.store(output_ptrs, softmax_output, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.cuda.current_device()\n",
    "properties = driver.active.utils.get_device_properties(device)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "target = triton.runtime.driver.active.get_current_target()\n",
    "kernels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "\n",
    "    # Another trick we can use is to ask the compiler to use more threads per row by\n",
    "    # increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "    # You will see in the next tutorial how to auto-tune this value in a more natural\n",
    "    # way so you don't have to come up with manual heuristics yourself.\n",
    "    num_warps = 8\n",
    "\n",
    "    # Number of software piepling stages.\n",
    "    num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "\n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    # pre-compile kernel to get register usage and compute thread occupancy.\n",
    "    kernel, num_programs = kernels.get(BLOCK_SIZE, (None, 0))\n",
    "    if kernel is None:\n",
    "        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,\n",
    "                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))\n",
    "        kernel._init_handles()\n",
    "        n_regs = kernel.n_regs\n",
    "        size_smem = kernel.metadata.shared\n",
    "        if is_hip():\n",
    "            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.\n",
    "            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.\n",
    "            # ISA SECTION (3.6.4 for CDNA3)\n",
    "            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used\n",
    "            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total\n",
    "            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is\n",
    "            # not required to be equal numbers of both types.\n",
    "            if is_cdna():\n",
    "                NUM_GPRS = NUM_REGS * 2\n",
    "\n",
    "            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.\n",
    "            # When we divide this number with WARP_SIZE we get maximum number of waves that can\n",
    "            # execute on a CU (multi-processor)  in parallel.\n",
    "            MAX_NUM_THREADS = properties[\"max_threads_per_sm\"]\n",
    "            max_num_waves = MAX_NUM_THREADS // WARP_SIZE\n",
    "            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps\n",
    "        else:\n",
    "            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "        occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "        num_programs = NUM_SM * occupancy\n",
    "        kernels[BLOCK_SIZE] = (kernel, num_programs)\n",
    "\n",
    "    num_programs = min(num_programs, n_rows)\n",
    "\n",
    "    # Create a number of persistent programs.\n",
    "    kernel[(num_programs, 1, 1)](\n",
    "        y,\n",
    "        x,\n",
    "        x.stride(0),\n",
    "        y.stride(0),\n",
    "        n_rows,\n",
    "        n_cols,\n",
    "    )\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1823\u001b[39m, \u001b[38;5;241m781\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m y_triton \u001b[38;5;241m=\u001b[39m softmax(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/nn/lib/python3.11/site-packages/torch/random.py:46\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[0;32m~/miniconda3/envs/nn/lib/python3.11/site-packages/torch/cuda/random.py:127\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    124\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    125\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 127\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nn/lib/python3.11/site-packages/torch/cuda/__init__.py:244\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[0;32m~/miniconda3/envs/nn/lib/python3.11/site-packages/torch/cuda/random.py:125\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[1;32m    124\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 125\u001b[0m     default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randn(1823, 781, device='cuda')\n",
    "y_triton = softmax(x)\n",
    "y_torch = torch.softmax(x, axis=1)\n",
    "assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m     gbps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m ms: \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39melement_size() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-9\u001b[39m \u001b[38;5;241m/\u001b[39m (ms \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gbps(ms)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshow_plots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nn/lib/python3.11/site-packages/triton/testing.py:346\u001b[0m, in \u001b[0;36mMark.run\u001b[0;34m(self, show_plots, print_data, save_path, return_df, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m     html\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<html><body>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bench \u001b[38;5;129;01min\u001b[39;00m benchmarks:\n\u001b[0;32m--> 346\u001b[0m     result_dfs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbench\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_plots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_path:\n\u001b[1;32m    348\u001b[0m         html\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<image src=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mbench\u001b[38;5;241m.\u001b[39mplot_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m/>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nn/lib/python3.11/site-packages/triton/testing.py:289\u001b[0m, in \u001b[0;36mMark._run\u001b[0;34m(self, bench, save_path, show_plots, print_data, diff_col, save_precision, **kwrags)\u001b[0m\n\u001b[1;32m    287\u001b[0m row_mean, row_min, row_max \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m bench\u001b[38;5;241m.\u001b[39mline_vals:\n\u001b[0;32m--> 289\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mbench\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline_arg\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbench\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwrags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m         y_mean, y_min, y_max \u001b[38;5;241m=\u001b[39m ret\n",
      "Cell \u001b[0;32mIn[20], line 19\u001b[0m, in \u001b[0;36mbenchmark\u001b[0;34m(M, N, provider)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;129m@triton\u001b[39m\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39mperf_report(\n\u001b[1;32m      4\u001b[0m     triton\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39mBenchmark(\n\u001b[1;32m      5\u001b[0m         x_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# argument names to use as an x-axis for the plot\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     ))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbenchmark\u001b[39m(M, N, provider):\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     stream \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mStream()\n\u001b[1;32m     21\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_stream(stream)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=['triton', 'torch'],  # possible values for `line_arg``\n",
    "        line_names=[\n",
    "            \"Triton\",\n",
    "            \"Torch\",\n",
    "        ],  # label name for the lines\n",
    "        styles=[('blue', '-'), ('green', '-')],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"softmax-performance\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n",
    "    ))\n",
    "def benchmark(M, N, provider):\n",
    "    x = torch.randn(M, N, device='cuda', dtype=torch.float32)\n",
    "    stream = torch.cuda.Stream()\n",
    "    torch.cuda.set_stream(stream)\n",
    "    if provider == 'torch':\n",
    "        ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n",
    "    if provider == 'triton':\n",
    "        ms = triton.testing.do_bench(lambda: softmax(x))\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
