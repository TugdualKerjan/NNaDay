{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a module that represents a GPT2 model with a relevant config file, we can train it ! We will use tinystories to train it as per the tutorial here: https://huggingface.co/blog/sachithgunasekara/nanojaxgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jax.numpy as np\n",
    "from GPT2 import GPTConfig\n",
    "import numpy\n",
    "\n",
    "data_dir = \"dataset\"\n",
    "config = GPTConfig()\n",
    "\n",
    "def get_batch(split: str):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = numpy.memmap(os.path.join(data_dir, 'train.bin'), dtype=numpy.uint16, mode='r')\n",
    "    else:\n",
    "        data = numpy.memmap(os.path.join(data_dir, 'validation.bin'), dtype=numpy.uint16, mode='r')\n",
    "\n",
    "    ix = numpy.random.randint(len(data) - config.block_size, size=(8,))\n",
    "    x = np.stack([np.array(data[i:i + config.block_size], dtype=np.int64) for i in ix])\n",
    "    y = np.stack([np.array(data[i + 1:i + 1 + config.block_size], dtype=np.int64) for i in ix])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax, jax\n",
    "import equinox as eqx\n",
    "\n",
    "learning_rate = 1e-5\n",
    "warmup_iters = 10\n",
    "init_from = \"scratch\"\n",
    "lr_decay_iters = 20\n",
    "iter_num = 0\n",
    "min_lr = 1e-6\n",
    "\n",
    "lr_scheduler = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=learning_rate,\n",
    "    warmup_steps=warmup_iters if init_from == 'scratch' else 0,\n",
    "    decay_steps=lr_decay_iters - iter_num,\n",
    "    end_value=min_lr,\n",
    ")\n",
    "\n",
    "optimizer = optax.inject_hyperparams(optax.adamw)(learning_rate=learning_rate)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def loss(model, x, y):\n",
    "    logits = jax.vmap(model)(x)\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=y)\n",
    "    return jax.numpy.mean(loss)\n",
    "\n",
    "def make_step(model, optimizer_state, x, y):\n",
    "    losses, grads = eqx.filter_value_and_grad(loss)(model, x, y)\n",
    "    updates, optimizer_state = optimizer.update(grads, optimizer_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, optimizer_state, losses\n",
    "\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model = eqx.nn.inference_mode(model)\n",
    "    for split in ['train', 'val']:\n",
    "        losses = jax.numpy.zeros(10)\n",
    "        for k in range(10):\n",
    "            x, y = get_batch(split)\n",
    "            loss = loss(model, jax.lax.stop_gradient(x), y)\n",
    "            losses = losses.at[k].set(loss.item())\n",
    "        out[split] = jax.numpy.mean(losses)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2 import GPT\n",
    "import wandb\n",
    "# init a new model from scratch\n",
    "print(\"Initializing a new model from scratch\")\n",
    "# determine the vocab size we'll use for from-scratch training\n",
    "key = jax.random.PRNGKey(69)\n",
    "\n",
    "gptconf = GPTConfig()\n",
    "model = GPT(gptconf, key)\n",
    "# convert_model_to_dtype()\n",
    "\n",
    "\n",
    "optimizer_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "for local_iter_num in range(100):\n",
    "    x, y = get_batch(\"train\")\n",
    "\n",
    "    model, optimizer_state, loss = make_step(model, optimizer_state, x, y)\n",
    "    print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = datasets.load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].select([i for i in range(0, 1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    # print(example)\n",
    "    return {\"tokenized\": [tokenizer.tokenize(x) for x in example[\"text\"]]}\n",
    "\n",
    "tokenized_data = dataset.map(tokenize, remove_columns=[\"text\"], batched=True, batch_size=8)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(dataset), 8):\n",
    "    data = tokenized_data[\"validation\"].select([i for i in range(i, i + 8)])[\"tokenized\"]\n",
    "    print(data)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tugdual/NNaDay/GPT2_with_JAX/GPT2.py:133: UserWarning: A JAX array is being set as static! This can result in unexpected behavior and is usually a mistake to do.\n",
      "  self.attn = CausalSelfAttention(config, key=key1)\n",
      "/home/tugdual/miniconda3/envs/jaxtts/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:6518: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in arange is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  output = _arange(start, stop=stop, step=step, dtype=dtype)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39marray([data[j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m: j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39minput_size] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i, i\u001b[38;5;241m+\u001b[39mbatch_size)])\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Perform a training step\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m model, optimizer_state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmake_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m, in \u001b[0;36mmake_step\u001b[0;34m(model, optimizer_state, x, y)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_step\u001b[39m(model, optimizer_state, x, y):\n\u001b[1;32m     15\u001b[0m     losses, grads \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mfilter_value_and_grad(loss)(model, x, y)\n\u001b[0;32m---> 16\u001b[0m     updates, optimizer_state \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     model \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mapply_updates(model, updates)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, optimizer_state, losses\n",
      "File \u001b[0;32m~/miniconda3/envs/jaxtts/lib/python3.11/site-packages/optax/transforms/_combining.py:73\u001b[0m, in \u001b[0;36mchain.<locals>.update_fn\u001b[0;34m(updates, state, params, **extra_args)\u001b[0m\n\u001b[1;32m     71\u001b[0m new_state \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s, fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(state, update_fns):\n\u001b[0;32m---> 73\u001b[0m   updates, new_s \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m   new_state\u001b[38;5;241m.\u001b[39mappend(new_s)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m updates, \u001b[38;5;28mtuple\u001b[39m(new_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/jaxtts/lib/python3.11/site-packages/optax/_src/base.py:330\u001b[0m, in \u001b[0;36mwith_extra_args_support.<locals>.update\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(updates, state, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_args):\n\u001b[1;32m    329\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m extra_args\n\u001b[0;32m--> 330\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jaxtts/lib/python3.11/site-packages/optax/_src/transform.py:230\u001b[0m, in \u001b[0;36mscale_by_adam.<locals>.update_fn\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    224\u001b[0m   mu_hat \u001b[38;5;241m=\u001b[39m jtu\u001b[38;5;241m.\u001b[39mtree_map(\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m m, g: b1 \u001b[38;5;241m*\u001b[39m m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m b1) \u001b[38;5;241m*\u001b[39m g,\n\u001b[1;32m    226\u001b[0m       otu\u001b[38;5;241m.\u001b[39mtree_bias_correction(\n\u001b[1;32m    227\u001b[0m           mu, b1, numerics\u001b[38;5;241m.\u001b[39msafe_int32_increment(count_inc)),\n\u001b[1;32m    228\u001b[0m       otu\u001b[38;5;241m.\u001b[39mtree_bias_correction(updates, b1, count_inc))\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m   mu_hat \u001b[38;5;241m=\u001b[39m \u001b[43motu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_bias_correction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount_inc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Dozat 2016 https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# Algorithm 2 further multiplies Adam's standard nu_hat by b2. It is\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# unclear why. Other Nadam implementations also omit the extra b2 factor.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m nu_hat \u001b[38;5;241m=\u001b[39m otu\u001b[38;5;241m.\u001b[39mtree_bias_correction(nu, b2, count_inc)\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jaxtts/lib/python3.11/site-packages/jax/_src/array.py:293\u001b[0m, in \u001b[0;36mArrayImpl.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m   \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_bool_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value)\n",
      "File \u001b[0;32m~/miniconda3/envs/jaxtts/lib/python3.11/site-packages/jax/_src/core.py:676\u001b[0m, in \u001b[0;36mcheck_bool_conversion\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m    673\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of an empty array is ambiguous. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    674\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `array.size > 0` to check that an array is not empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 676\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of an array with more than one element\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    677\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is ambiguous. Use a.any() or a.all()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import optax\n",
    "import jax.numpy as np\n",
    "import equinox as eqx\n",
    "from GPT2 import GPTConfig, GPT\n",
    "\n",
    "# Loss function with vmap to calculate loss for the entire batch\n",
    "def loss(model, x, y):\n",
    "    logits = jax.vmap(model)(x)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=y)\n",
    "    return jax.numpy.mean(loss)\n",
    "\n",
    "# Optimization step function\n",
    "def make_step(model, optimizer_state, x, y):\n",
    "    losses, grads = eqx.filter_value_and_grad(loss)(model, x, y)\n",
    "    updates, optimizer_state = optimizer.update(grads, optimizer_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, optimizer_state, losses\n",
    "\n",
    "# Set up GPT configuration and data\n",
    "config = GPTConfig()\n",
    "data = jax.numpy.cos(jax.numpy.arange(0, 100000) * (jax.numpy.pi)/1000)\n",
    "data = jax.numpy.ceil(data * config.vocab_size).astype(jax.numpy.int16)\n",
    "\n",
    "batch_size = 10\n",
    "input_size = 100\n",
    "\n",
    "# Initialize model and optimizer\n",
    "key = jax.random.PRNGKey(79)\n",
    "model = GPT(config, key)\n",
    "trainable = eqx.filter(model, eqx.is_array)\n",
    "optimizer = optax.adamw(learning_rate=1e-5)\n",
    "optimizer_state = optimizer.init(trainable)\n",
    "\n",
    "# Training loop\n",
    "for i in range(0, len(data) - input_size - 1, batch_size):\n",
    "    batch = jax.numpy.array([data[j: j+input_size] for j in range(i, i+batch_size)])\n",
    "    batch_y = jax.numpy.array([data[j+1: j+1+input_size] for j in range(i, i+batch_size)])\n",
    "    \n",
    "    # Perform a training step\n",
    "    model, optimizer_state, loss = make_step(model, optimizer_state, batch, batch_y)\n",
    "    print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, a = plt.subplots(1)\n",
    "a.plot(data)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxtts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
