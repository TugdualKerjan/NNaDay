{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "\n",
    "class Dataloader:\n",
    "    def __init__(self, dataset_path, sample_rate):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.list_of_files = list(Path(dataset_path).rglob(\"*.wav\"))\n",
    "    \n",
    "    def load_and_process(self, file_path):\n",
    "        # Load file and check sample rate\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        if sr != self.sample_rate:\n",
    "            y = librosa.resample(y, orig_sr=sr, target_sr=self.sample_rate)\n",
    "        \n",
    "        # Generate a random start position for segment extraction\n",
    "        random_start = random.randint(0, max(len(y) - self.sample_rate, 0))\n",
    "        y_segment = y[random_start : random_start + self.sample_rate]\n",
    "        \n",
    "        return np.array(y_segment)\n",
    "    \n",
    "    def get_batch(self, ids):\n",
    "        # Load each file individually, outside of JAX operations\n",
    "        batch = [self.load_and_process(self.list_of_files[id]) for id in ids]\n",
    "        \n",
    "        # Convert list of arrays to a JAX array with batch dimension\n",
    "        batch = jax.numpy.stack(batch)\n",
    "        \n",
    "        # Add extra dimension for compatibility with batch processing\n",
    "        batch = jax.numpy.expand_dims(batch, -2)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_of_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses: 6.066703796386719\n",
      "[ 0.00205232  0.00102228 -0.00179449 ...  0.00335476  0.00053784\n",
      "  0.00472914]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "from encodec import EncodecModel\n",
    "\n",
    "@eqx.filter_jit\n",
    "@eqx.filter_value_and_grad(has_aux=True)\n",
    "def calculate_loss(model, x):\n",
    "    y = jax.vmap(model)(x)\n",
    "\n",
    "    # MSE\n",
    "    loss = jax.numpy.linalg.norm((x - y), axis=-1)\n",
    "    # loss = jax.numpy.mean((x - y) ** 2, axis=-1)\n",
    "\n",
    "    loss = jax.numpy.mean(loss)\n",
    "    # commit_loss = jax.numpy.mean((z_e - jax.lax.stop_gradient(z_q)) ** 2)\n",
    "    # loss += commit_loss\n",
    "    return loss, y\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, optimizer, opt_state, x):\n",
    "    global i\n",
    "    (losses, y), grads = calculate_loss(model, x)\n",
    "    # print_per_layer(model)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, eqx.filter(model, eqx.is_array))\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    # print_per_layer(model, \"Model\", i)\n",
    "    # print_per_layer(updates, \"Updates\", i)\n",
    "    # print_per_layer(grads, \"Grads\", i)\n",
    "    # i+=1\n",
    "    return losses, y, model, opt_state\n",
    "\n",
    "# Return truncated weight array\n",
    "def trunc_init(weight: jax.Array, key: jax.random.PRNGKey) -> jax.Array:\n",
    "    print(weight.shape)\n",
    "    out, in_, three = weight.shape\n",
    "    \n",
    "    # Calculate fan_in and fan_out\n",
    "    fan_in = in_\n",
    "    fan_out = out\n",
    "    stddev = jax.numpy.sqrt(2.0 / (fan_in)) * 0.001\n",
    "    \n",
    "    return stddev * jax.random.normal(key, shape=(out, in_, three))\n",
    "\n",
    "def init_conv_weight(model, init_fn, key):\n",
    "    is_conv = lambda x: isinstance(x, (eqx.nn.Conv1d, eqx.nn.ConvTranspose1d))\n",
    "    get_weights = lambda m: [x.weight\n",
    "                            for x in jax.tree_util.tree_leaves(m, is_leaf=is_conv)\n",
    "                            if is_conv(x)]\n",
    "    weights = get_weights(model)\n",
    "    new_weights = [init_fn(weight, subkey)\n",
    "                    for weight, subkey in zip(weights, jax.random.split(key, len(weights)))]\n",
    "    new_model = eqx.tree_at(get_weights, model, new_weights)\n",
    "    return new_model\n",
    "\n",
    "def print_per_layer(model, tree_name, batch_idx):\n",
    "    # Function to identify Conv1d and ConvTranspose1d layers\n",
    "    is_conv = lambda x: isinstance(x, (eqx.nn.Conv1d, eqx.nn.ConvTranspose1d))\n",
    "    \n",
    "    # Extract weights from layers identified as Conv1d or ConvTranspose1d\n",
    "    get_weights = lambda m: [x.weight\n",
    "                             for x in jax.tree_util.tree_leaves(m, is_leaf=is_conv)\n",
    "                             if is_conv(x)]\n",
    "    \n",
    "    weights = get_weights(model)\n",
    "\n",
    "    # Log weights for each layer separately in TensorBoard\n",
    "    with train_summary_writer.as_default():\n",
    "        for i, weight in enumerate(weights):\n",
    "            flat_weight = jax.numpy.reshape(weight, -1)\n",
    "            tf.summary.histogram(f'{tree_name}/layer_{i}_weights', flat_weight, step=batch_idx)\n",
    "    \n",
    "    # print(\"Logged histograms for each layerâ€™s weights to TensorBoard.\")\n",
    "\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "learning_rate = 3e-2\n",
    "\n",
    "key = jax.random.PRNGKey(2)\n",
    "\n",
    "grab, key = jax.random.split(key, 2)\n",
    "\n",
    "model = EncodecModel(activation=jax.nn.silu, n_res_layers=0, key=grab)\n",
    "\n",
    "# sprint_per_layer(model)\n",
    "model = init_conv_weight(model, trunc_init, grab)\n",
    "# print_per_layer(model)\n",
    "optimizer = optax.chain(\n",
    "    optax.adam(learning_rate, b1=0.5, b2=0.9),\n",
    "    # optax.add_decayed_weights(0.01)\n",
    ")\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "dataset = Dataloader(\"/home/tugdual/JAXTTS/ResNet/wav\", sample_rate=24000)\n",
    "\n",
    "\n",
    "plt.ion()  # Turn on interactive mode\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "line_input, = ax.step([], [], label='Input', where='mid')\n",
    "line_output, = ax.step([], [], label='Output', where='mid')\n",
    "ax.set_title('Model Input and Output')\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    grab, key = jax.random.split(key, 2)\n",
    "    perm = jax.random.permutation(grab, len(dataset))\n",
    "\n",
    "    for batch_idx, batch in enumerate(range(0, len(dataset), batch_size)):\n",
    "        ids = perm[batch : batch + batch_size]\n",
    "        batch = dataset.get_batch(ids)\n",
    "        losses, y, model, opt_state = make_step(model, optimizer, opt_state, batch)\n",
    "        x = batch[0, 0]\n",
    "        y = y[0, 0]\n",
    "        # break\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', losses, step=batch_idx)\n",
    "        if batch_idx % 16 == 0:\n",
    "            line_input.set_xdata(np.arange(len(x)))\n",
    "            line_input.set_ydata(x)\n",
    "            line_output.set_xdata(np.arange(len(y)))\n",
    "            line_output.set_ydata(y)\n",
    "\n",
    "            # Set the limits\n",
    "            ax.set_xlim(0, len(x))\n",
    "            ax.set_ylim(-1, 1)\n",
    "\n",
    "            # Redraw the figure\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Losses: {losses}\")\n",
    "            print(y)\n",
    "            plt.draw()\n",
    "            plt.pause(0.1)\n",
    "            display(fig)  # Redisplay the figure after updating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncodecModel(\n",
      "  encoder=Encoder(\n",
      "    B_layers=[\n",
      "      EncoderLayer(\n",
      "        resblocks=[\n",
      "          ResBlock(\n",
      "            conv1=Conv1d(\n",
      "              num_spatial_dims=1,\n",
      "              weight=f32[32,32,3],\n",
      "              bias=f32[32,1],\n",
      "              in_channels=32,\n",
      "              out_channels=32,\n",
      "              kernel_size=(3,),\n",
      "              stride=(1,),\n",
      "              padding='SAME',\n",
      "              dilation=(1,),\n",
      "              groups=1,\n",
      "              use_bias=True,\n",
      "              padding_mode='ZEROS'\n",
      "            ),\n",
      "            conv2=Conv1d(\n",
      "              num_spatial_dims=1,\n",
      "              weight=f32[32,32,3],\n",
      "              bias=f32[32,1],\n",
      "              in_channels=32,\n",
      "              out_channels=32,\n",
      "              kernel_size=(3,),\n",
      "              stride=(1,),\n",
      "              padding='SAME',\n",
      "              dilation=(1,),\n",
      "              groups=1,\n",
      "              use_bias=True,\n",
      "              padding_mode='ZEROS'\n",
      "            ),\n",
      "            activation=<wrapped function relu>,\n",
      "            norm=<class 'equinox.nn._normalisation.LayerNorm'>\n",
      "          )\n",
      "        ],\n",
      "        conv=Conv1d(\n",
      "          num_spatial_dims=1,\n",
      "          weight=f32[64,32,4],\n",
      "          bias=f32[64,1],\n",
      "          in_channels=32,\n",
      "          out_channels=64,\n",
      "          kernel_size=(4,),\n",
      "          stride=(2,),\n",
      "          padding='SAME',\n",
      "          dilation=(1,),\n",
      "          groups=1,\n",
      "          use_bias=True,\n",
      "          padding_mode='ZEROS'\n",
      "        ),\n",
      "        activation=<wrapped function relu>,\n",
      "        norm=LayerNorm(\n",
      "          shape=(64, 12000),\n",
      "          eps=1e-05,\n",
      "          use_weight=True,\n",
      "          use_bias=True,\n",
      "          weight=f32[64,12000],\n",
      "          bias=f32[64,12000]\n",
      "        )\n",
      "      ),\n",
      "      EncoderLayer(\n",
      "        resblocks=[\n",
      "          ResBlock(\n",
      "            conv1=Conv1d(\n",
      "              num_spatial_dims=1,\n",
      "              weight=f32[64,64,3],\n",
      "              bias=f32[64,1],\n",
      "              in_channels=64,\n",
      "              out_channels=64,\n",
      "              kernel_size=(3,),\n",
      "              stride=(1,),\n",
      "              padding='SAME',\n",
      "              dilation=(1,),\n",
      "              groups=1,\n",
      "              use_bias=True,\n",
      "              padding_mode='ZEROS'\n",
      "            ),\n",
      "            conv2=Conv1d(\n",
      "              num_spatial_dims=1,\n",
      "              weight=f32[64,64,3],\n",
      "              bias=f32[64,1],\n",
      "              in_channels=64,\n",
      "              out_channels=64,\n",
      "              kernel_size=(3,),\n",
      "              stride=(1,),\n",
      "              padding='SAME',\n",
      "              dilation=(1,),\n",
      "              groups=1,\n",
      "              use_bias=True,\n",
      "              padding_mode='ZEROS'\n",
      "            ),\n",
      "            activation=<wrapped function relu>,\n",
      "            norm=<class 'equinox.nn._normalisation.LayerNorm'>\n",
      "          )\n",
      "        ],\n",
      "        conv=Conv1d(\n",
      "          num_spatial_dims=1,\n",
      "          weight=f32[128,64,8],\n",
      "          bias=f32[128,1],\n",
      "          in_channels=64,\n",
      "          out_channels=128,\n",
      "          kernel_size=(8,),\n",
      "          stride=(4,),\n",
      "          padding='SAME',\n",
      "          dilation=(1,),\n",
      "          groups=1,\n",
      "          use_bias=True,\n",
      "          padding_mode='ZEROS'\n",
      "        ),\n",
      "        activation=<wrapped function relu>,\n",
      "        norm=LayerNorm(\n",
      "          shape=(128, 3000),\n",
      "          eps=1e-05,\n",
      "          use_weight=True,\n",
      "          use_bias=True,\n",
      "          weight=f32[128,3000],\n",
      "          bias=f32[128,3000]\n",
      "        )\n",
      "      ),\n",
      "      EncoderLayer(\n",
      "        resblocks=[\n",
      "          ResBlock(\n",
      "            conv1=Conv1d(\n",
      "              num_spatial_dims=1,\n",
      "              weight=f32[128,128,3],\n",
      "              bias=f32[128,1],\n",
      "              in_channels=128,\n",
      "              out_channels=128,\n",
      "              kernel_size=(3,),\n",
      "              stride=(1,),\n",
      "              padding='SAME',\n",
      "              dilation=(1,),\n",
      "              groups=1,\n",
      "              use_bias=True,\n",
      "              padding_mode='ZEROS'\n",
      "            ),\n",
      "            conv2=Conv1d(\n",
      "              num_spatial_dims=1,\n",
      "              weight=f32[128,128,3],\n",
      "              bias=f32[128,1],\n",
      "              in_channels=128,\n",
      "              out_channels=128,\n",
      "              kernel_size=(3,),\n",
      "              stride=(1,),\n",
      "              padding='SAME',\n",
      "              dilation=(1,),\n",
      "              groups=1,\n",
      "              use_bias=True,\n",
      "              padding_mode='ZEROS'\n",
      "            ),\n",
      "            activation=<wrapped function relu>,\n",
      "            norm=<class 'equinox.nn._normalisation.LayerNorm'>\n",
      "          )\n",
      "        ],\n",
      "        conv=Conv1d(\n",
      "          num_spatial_dims=1,\n",
      "          weight=f32[256,128,10],\n",
      "          bias=f32[256,1],\n",
      "          in_channels=128,\n",
      "          out_channels=256,\n",
      "          kernel_size=(10,),\n",
      "          stride=(5,),\n",
      "          padding='SAME',\n",
      "          dilation=(1,),\n",
      "          groups=1,\n",
      "          use_bias=True,\n",
      "          padding_mode='ZEROS'\n",
      "        ),\n",
      "        activation=<wrapped function relu>,\n",
      "        norm=LayerNorm(\n",
      "          shape=(256, 600),\n",
      "          eps=1e-05,\n",
      "          use_weight=True,\n",
      "          use_bias=True,\n",
      "          weight=f32[256,600],\n",
      "          bias=f32[256,600]\n",
      "        )\n",
      "      ),\n",
      "      EncoderLayer(\n",
      "        resblocks=[\n",
      "          ResBlock(\n",
      "            conv1=Conv1d(\n",
      "              num_spatial_dims=1,\n",
      "              weight=f32[256,256,3],\n",
      "              bias=f32[256,1],\n",
      "              in_channels=256,\n",
      "              out_channels=256,\n",
      "              kernel_size=(3,),\n",
      "              stride=(1,),\n",
      "              padding='SAME',\n",
      "              dilation=(1,),\n",
      "              groups=1,\n",
      "              use_bias=True,\n",
      "              padding_mode='ZEROS'\n",
      "            ),\n",
      "            conv2=Conv1d(\n",
      "              num_spatial_dims=1,\n",
      "              weight=f32[256,256,3],\n",
      "              bias=f32[256,1],\n",
      "              in_channels=256,\n",
      "              out_channels=256,\n",
      "              kernel_size=(3,),\n",
      "              stride=(1,),\n",
      "              padding='SAME',\n",
      "              dilation=(1,),\n",
      "              groups=1,\n",
      "              use_bias=True,\n",
      "              padding_mode='ZEROS'\n",
      "            ),\n",
      "            activation=<wrapped function relu>,\n",
      "            norm=<class 'equinox.nn._normalisation.LayerNorm'>\n",
      "          )\n",
      "        ],\n",
      "        conv=Conv1d(\n",
      "          num_spatial_dims=1,\n",
      "          weight=f32[512,256,16],\n",
      "          bias=f32[512,1],\n",
      "          in_channels=256,\n",
      "          out_channels=512,\n",
      "          kernel_size=(16,),\n",
      "          stride=(8,),\n",
      "          padding='SAME',\n",
      "          dilation=(1,),\n",
      "          groups=1,\n",
      "          use_bias=True,\n",
      "          padding_mode='ZEROS'\n",
      "        ),\n",
      "        activation=<wrapped function relu>,\n",
      "        norm=LayerNorm(\n",
      "          shape=(512, 75),\n",
      "          eps=1e-05,\n",
      "          use_weight=True,\n",
      "          use_bias=True,\n",
      "          weight=f32[512,75],\n",
      "          bias=f32[512,75]\n",
      "        )\n",
      "      )\n",
      "    ],\n",
      "    activation=<wrapped function elu>,\n",
      "    first=Conv1d(\n",
      "      num_spatial_dims=1,\n",
      "      weight=f32[32,1,7],\n",
      "      bias=f32[32,1],\n",
      "      in_channels=1,\n",
      "      out_channels=32,\n",
      "      kernel_size=(7,),\n",
      "      stride=(1,),\n",
      "      padding='SAME',\n",
      "      dilation=(1,),\n",
      "      groups=1,\n",
      "      use_bias=True,\n",
      "      padding_mode='ZEROS'\n",
      "    ),\n",
      "    last=Conv1d(\n",
      "      num_spatial_dims=1,\n",
      "      weight=f32[128,512,7],\n",
      "      bias=f32[128,1],\n",
      "      in_channels=512,\n",
      "      out_channels=128,\n",
      "      kernel_size=(7,),\n",
      "      stride=(1,),\n",
      "      padding='SAME',\n",
      "      dilation=(1,),\n",
      "      groups=1,\n",
      "      use_bias=True,\n",
      "      padding_mode='ZEROS'\n",
      "    ),\n",
      "    norm_first=LayerNorm(\n",
      "      shape=(32, 24000),\n",
      "      eps=1e-05,\n",
      "      use_weight=True,\n",
      "      use_bias=True,\n",
      "      weight=f32[32,24000],\n",
      "      bias=f32[32,24000]\n",
      "    ),\n",
      "    norm_last=LayerNorm(\n",
      "      shape=(128, 75),\n",
      "      eps=1e-05,\n",
      "      use_weight=True,\n",
      "      use_bias=True,\n",
      "      weight=f32[128,75],\n",
      "      bias=f32[128,75]\n",
      "    )\n",
      "  ),\n",
      "  decoder=Decoder(\n",
      "    B_layers=[\n",
      "      DecoderLayer(\n",
      "        resblocks=[\n",
      "          ResBlock(\n",
      "            conv1=Conv1d(\n",
      "              num_spatial_dims=1,\n",
      "              weight=f32[256,256,3],\n",
      "              bias=f32[256,1],\n",
      "              in_channels=256,\n",
      "              out_channels=256,\n",
      "              kernel_size=(3,),\n",
      "              stride=(1,),\n",
      "              padding='SAME',\n",
      "              dilation=(1,),\n",
      "              groups=1,\n",
      "              use_bias=True,\n",
      "              padding_mode='ZEROS'\n",
      "            ),\n",
      "            conv2=Conv1d(\n",
      "              num_spatial_dims=1,\n",
      "              weight=f32[256,256,3],\n",
      "              bias=f32[256,1],\n",
      "              in_channels=256,\n",
      "              out_channels=256,\n",
      "              kernel_size=(3,),\n",
      "              stride=(1,),\n",
      "              padding='SAME',\n",
      "              dilation=(1,),\n",
      "              groups=1,\n",
      "              use_bias=True,\n",
      "              padding_mode='ZEROS'\n",
      "            ),\n",
      "            activation=<wrapped function elu>,\n",
      "            norm=<class 'equinox.nn._normalisation.LayerNorm'>\n",
      "          )\n",
      "        ],\n",
      "        conv=ConvTranspose1d(\n",
      "          num_spatial_dims=1,\n",
      "          weight=f32[256,512,16],\n",
      "          bias=f32[256,1],\n",
      "          in_channels=512,\n",
      "          out_channels=256,\n",
      "          kernel_size=(16,),\n",
      "          stride=(8,),\n",
      "          padding='SAME',\n",
      "          output_padding=(0,),\n",
      "          dilation=(1,),\n",
      "          groups=1,\n",
      "          use_bias=True,\n",
      "          padding_mode='ZEROS'\n",
      "        ),\n",
      "        activation=<wrapped function elu>,\n",
      "        norm=LayerNorm(\n",
      "          shape=(256, 600),\n",
      "          eps=1e-05,\n",
      "          use_weight=True,\n",
      "          use_bias=True,\n",
      "          weight=f32[256,600],\n",
      "          bias=f32[256,600]\n",
      "        )\n",
      "      ),\n",
      "      DecoderLayer(\n",
      "        resblocks=[\n",
      "          ResBlock(\n",
      "            conv1=Conv1d(\n",
      "              num_spatial_dims=1,\n",
      "              weight=f32[128,128,3],\n",
      "              bias=f32[128,1],\n",
      "              in_channels=128,\n",
      "              out_channels=128,\n",
      "              kernel_size=(3,),\n",
      "              stride=(1,),\n",
      "              padding='SAME',\n",
      "              dilation=(1,),\n",
      "              groups=1,\n",
      "              use_bias=True,\n",
      "              padding_mode='ZEROS'\n",
      "            ),\n",
      "            conv2=Conv1d(\n",
      "              num_spatial_dims=1,\n",
      "              weight=f32[128,128,3],\n",
      "              bias=f32[128,1],\n",
      "              in_channels=128,\n",
      "              out_channels=128,\n",
      "              kernel_size=(3,),\n",
      "              stride=(1,),\n",
      "              padding='SAME',\n",
      "              dilation=(1,),\n",
      "              groups=1,\n",
      "              use_bias=True,\n",
      "              padding_mode='ZEROS'\n",
      "            ),\n",
      "            activation=<wrapped function elu>,\n",
      "            norm=<class 'equinox.nn._normalisation.LayerNorm'>\n",
      "          )\n",
      "        ],\n",
      "        conv=ConvTranspose1d(\n",
      "          num_spatial_dims=1,\n",
      "          weight=f32[128,256,10],\n",
      "          bias=f32[128,1],\n",
      "          in_channels=256,\n",
      "          out_channels=128,\n",
      "          kernel_size=(10,),\n",
      "          stride=(5,),\n",
      "          padding='SAME',\n",
      "          output_padding=(0,),\n",
      "          dilation=(1,),\n",
      "          groups=1,\n",
      "          use_bias=True,\n",
      "          padding_mode='ZEROS'\n",
      "        ),\n",
      "        activation=<wrapped function elu>,\n",
      "        norm=LayerNorm(\n",
      "          shape=(128, 3000),\n",
      "          eps=1e-05,\n",
      "          use_weight=True,\n",
      "          use_bias=True,\n",
      "          weight=f32[128,3000],\n",
      "          bias=f32[128,3000]\n",
      "        )\n",
      "      ),\n",
      "      DecoderLayer(\n",
      "        resblocks=[\n",
      "          ResBlock(\n",
      "            conv1=Conv1d(\n",
      "              num_spatial_dims=1,\n",
      "              weight=f32[64,64,3],\n",
      "              bias=f32[64,1],\n",
      "              in_channels=64,\n",
      "              out_channels=64,\n",
      "              kernel_size=(3,),\n",
      "              stride=(1,),\n",
      "              padding='SAME',\n",
      "              dilation=(1,),\n",
      "              groups=1,\n",
      "              use_bias=True,\n",
      "              padding_mode='ZEROS'\n",
      "            ),\n",
      "            conv2=Conv1d(\n",
      "              num_spatial_dims=1,\n",
      "              weight=f32[64,64,3],\n",
      "              bias=f32[64,1],\n",
      "              in_channels=64,\n",
      "              out_channels=64,\n",
      "              kernel_size=(3,),\n",
      "              stride=(1,),\n",
      "              padding='SAME',\n",
      "              dilation=(1,),\n",
      "              groups=1,\n",
      "              use_bias=True,\n",
      "              padding_mode='ZEROS'\n",
      "            ),\n",
      "            activation=<wrapped function elu>,\n",
      "            norm=<class 'equinox.nn._normalisation.LayerNorm'>\n",
      "          )\n",
      "        ],\n",
      "        conv=ConvTranspose1d(\n",
      "          num_spatial_dims=1,\n",
      "          weight=f32[64,128,8],\n",
      "          bias=f32[64,1],\n",
      "          in_channels=128,\n",
      "          out_channels=64,\n",
      "          kernel_size=(8,),\n",
      "          stride=(4,),\n",
      "          padding='SAME',\n",
      "          output_padding=(0,),\n",
      "          dilation=(1,),\n",
      "          groups=1,\n",
      "          use_bias=True,\n",
      "          padding_mode='ZEROS'\n",
      "        ),\n",
      "        activation=<wrapped function elu>,\n",
      "        norm=LayerNorm(\n",
      "          shape=(64, 12000),\n",
      "          eps=1e-05,\n",
      "          use_weight=True,\n",
      "          use_bias=True,\n",
      "          weight=f32[64,12000],\n",
      "          bias=f32[64,12000]\n",
      "        )\n",
      "      ),\n",
      "      DecoderLayer(\n",
      "        resblocks=[\n",
      "          ResBlock(\n",
      "            conv1=Conv1d(\n",
      "              num_spatial_dims=1,\n",
      "              weight=f32[32,32,3],\n",
      "              bias=f32[32,1],\n",
      "              in_channels=32,\n",
      "              out_channels=32,\n",
      "              kernel_size=(3,),\n",
      "              stride=(1,),\n",
      "              padding='SAME',\n",
      "              dilation=(1,),\n",
      "              groups=1,\n",
      "              use_bias=True,\n",
      "              padding_mode='ZEROS'\n",
      "            ),\n",
      "            conv2=Conv1d(\n",
      "              num_spatial_dims=1,\n",
      "              weight=f32[32,32,3],\n",
      "              bias=f32[32,1],\n",
      "              in_channels=32,\n",
      "              out_channels=32,\n",
      "              kernel_size=(3,),\n",
      "              stride=(1,),\n",
      "              padding='SAME',\n",
      "              dilation=(1,),\n",
      "              groups=1,\n",
      "              use_bias=True,\n",
      "              padding_mode='ZEROS'\n",
      "            ),\n",
      "            activation=<wrapped function elu>,\n",
      "            norm=<class 'equinox.nn._normalisation.LayerNorm'>\n",
      "          )\n",
      "        ],\n",
      "        conv=ConvTranspose1d(\n",
      "          num_spatial_dims=1,\n",
      "          weight=f32[32,64,4],\n",
      "          bias=f32[32,1],\n",
      "          in_channels=64,\n",
      "          out_channels=32,\n",
      "          kernel_size=(4,),\n",
      "          stride=(2,),\n",
      "          padding='SAME',\n",
      "          output_padding=(0,),\n",
      "          dilation=(1,),\n",
      "          groups=1,\n",
      "          use_bias=True,\n",
      "          padding_mode='ZEROS'\n",
      "        ),\n",
      "        activation=<wrapped function elu>,\n",
      "        norm=LayerNorm(\n",
      "          shape=(32, 24000),\n",
      "          eps=1e-05,\n",
      "          use_weight=True,\n",
      "          use_bias=True,\n",
      "          weight=f32[32,24000],\n",
      "          bias=f32[32,24000]\n",
      "        )\n",
      "      )\n",
      "    ],\n",
      "    final_activation=Identity(),\n",
      "    activation=<wrapped function elu>,\n",
      "    first=ConvTranspose1d(\n",
      "      num_spatial_dims=1,\n",
      "      weight=f32[512,128,7],\n",
      "      bias=f32[512,1],\n",
      "      in_channels=128,\n",
      "      out_channels=512,\n",
      "      kernel_size=(7,),\n",
      "      stride=(1,),\n",
      "      padding='SAME',\n",
      "      output_padding=(0,),\n",
      "      dilation=(1,),\n",
      "      groups=1,\n",
      "      use_bias=True,\n",
      "      padding_mode='ZEROS'\n",
      "    ),\n",
      "    last=ConvTranspose1d(\n",
      "      num_spatial_dims=1,\n",
      "      weight=f32[1,32,7],\n",
      "      bias=f32[1,1],\n",
      "      in_channels=32,\n",
      "      out_channels=1,\n",
      "      kernel_size=(7,),\n",
      "      stride=(1,),\n",
      "      padding='SAME',\n",
      "      output_padding=(0,),\n",
      "      dilation=(1,),\n",
      "      groups=1,\n",
      "      use_bias=True,\n",
      "      padding_mode='ZEROS'\n",
      "    ),\n",
      "    norm_first=LayerNorm(\n",
      "      shape=(512, 75),\n",
      "      eps=1e-05,\n",
      "      use_weight=True,\n",
      "      use_bias=True,\n",
      "      weight=f32[512,75],\n",
      "      bias=f32[512,75]\n",
      "    ),\n",
      "    norm_last=LayerNorm(\n",
      "      shape=(1, 24000),\n",
      "      eps=1e-05,\n",
      "      use_weight=True,\n",
      "      use_bias=True,\n",
      "      weight=f32[1,24000],\n",
      "      bias=f32[1,24000]\n",
      "    )\n",
      "  )\n",
      ")\n",
      "(1, 1, 24000)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from encodec import EncodecModel\n",
    "\n",
    "key = jax.random.PRNGKey(1)\n",
    "model = EncodecModel(activation=jax.nn.elu,key=key)\n",
    "print(model)\n",
    "# x = jax.random.normal(key, shape=(1, 1, 24000))\n",
    "x = jax.numpy.ones((1, 1, 24000))\n",
    "print(x.shape)\n",
    "y = jax.vmap(model)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxtts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
